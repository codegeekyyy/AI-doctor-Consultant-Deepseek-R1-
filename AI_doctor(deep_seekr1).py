# -*- coding: utf-8 -*-
"""AI-doctor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tMNARcNfr6XVvTSLCznTiySlnUsWBLvf
"""

# install dependencies
!pip install unsloth # install unsloth
!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!
!pip install torch
!pip install trl
!pip install torchvision
!pip install peft
!pip install xformers

# import all libraries
from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer
from unsloth import is_bfloat16_supported
from huggingface_hub import login
from transformers import TrainingArguments
from datasets import load_dataset
import wandb

# check hf tokens
from google.colab import userdata
hf_token=userdata.get('hf_token')
login(token=hf_token)

# check gpu availability
import torch
print("cuda avail:",torch.cuda.is_available())
print("gpu device",torch.cuda.get_device_name(0) if torch.cuda.is_available() else "cpu")

# setup pretrained model
model_name="deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
max_seq_length=2048 # Corrected parameter name
dtype=None
load_in_4bit=True

model, tokenizer =FastLanguageModel.from_pretrained(
model_name=model_name,
max_seq_length=max_seq_length,
dtype=dtype,
load_in_4bit=load_in_4bit,
token=hf_token)

# Step6: Setup system prompt
prompt_style = """
Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.

Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.

### Task:
You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.

### Query:
{}

### Answer:
<think>{}
"""

# run inference on the model
# define a test question
question="""A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or
              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,
              what would cystometry most likely reveal about her residual volume and detrusor contractions?"""
FastLanguageModel.for_inference(model)
# tokenize the input
inputs=tokenizer([prompt_style.format(question, "")], return_tensors="pt")
inputs = {k: v.to("cuda") for k, v in inputs.items()}
# generate a response
output=model.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=1200,
    use_cache=True
)

# decode the response tokens back to text
reponse=tokenizer.decode(output[0], skip_special_tokens=True)
print(reponse[0])

# now fine-tuning
# load-dataset
medical_dataset=load_dataset("FreedomIntelligence/medical-o1-reasoning-SFT", "en", split = "train[:500]", trust_remote_code=True)

medical_dataset[0]

# add a token beg of sentence and end of sentence
EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which tells the model when to stop generating text during training
EOS_TOKEN

### Finetuning
# Updated training prompt style to add  tag
train_prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.
Please answer the following medical question.

### Question:
{}

### Response:
<think>
{}
</think>
{}
"""

# prepare the data for fine tuning
def preprocess_input_data(examples):
  inputs=examples['Question']
  cots=examples['Complex_CoT']
  outputs=examples['Response']
  texts=[]
  for input,cot,output in zip(inputs,cots,outputs):
    text=train_prompt_style.format(input,cot,output) + EOS_TOKEN
    texts.append(text)


  return{
      "texts": texts,
  }

finetune_dataset=medical_dataset.map(preprocess_input_data, batched=True)

finetune_dataset['texts'][0]

# setup/apply lora fintuning to the model
model_lora=FastLanguageModel.get_peft_model(
    model=model,
    r=16,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3047,
    use_rslora=False,
    loftq_config=None
)

def formatting_func(example):
    if isinstance(example["texts"], list):
        return example["texts"]
    else:
        return [example["texts"]]

trainer = SFTTrainer(
    model = model_lora,
    tokenizer = tokenizer,
    train_dataset = finetune_dataset,
    formatting_func = formatting_func,   # ðŸ‘ˆ now safe for batching
    max_seq_length = max_sequence_length,
    dataset_num_proc = 1,

    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        num_train_epochs = 1,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 10,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir = "outputs",
    ),
)

from google.colab import userdata
wnb_token=userdata.get("wandb")
# login to wnb
wandb.login(key=wnb_token)#import wnb
run=wandb.init(
    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B',
    job_type='training',
    anonymous='allow'
)

trainer_stats=trainer.train()

wandb.finish()

# testing after fine tuning
question = """A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing
              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,
              what would cystometry most likely reveal about her residual volume and detrusor contractions?"""
FastLanguageModel.for_inference(model_lora)
# tokenize the input
inputs=tokenizer([prompt_style.format(question, "")], return_tensors="pt")
inputs = {k: v.to("cuda") for k, v in inputs.items()}
# generate a response
output=model_lora.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=1200,
    use_cache=True
)

# decode the response tokens back to text
reponse=tokenizer.decode(output[0], skip_special_tokens=True)
print(reponse)

print(reponse.split("### Answer:")[1])

question = """A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,
              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative,
              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.
              What is the most likely predisposing factor for this patient's condition?"""

FastLanguageModel.for_inference(model_lora)

# Tokenize the input
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generate a response
outputs = model_lora.generate (
    input_ids = inputs.input_ids,
    attention_mask = inputs.attention_mask,
    max_new_tokens = 1200,
    use_cache = True
)

# Decode the response tokens back to text
reponse = tokenizer.batch_decode(outputs)

print(reponse[0].split("### Answer:")[1])

# In your Google Colab Notebook, after fine-tuning...
# The following code is for saving the model to a directory
# which can then be downloaded.
model_lora.save_pretrained("deepseek_r1_medical_finetune")
tokenizer.save_pretrained("deepseek_r1_medical_finetune")

# You can download the entire folder as a zip file.
# To do so, you'll need to use the following commands:
!zip -r /content/deepseek_r1_medical_finetune.zip /content/deepseek_r1_medical_finetune
from google.colab import files
files.download("/content/deepseek_r1_medical_finetune.zip")

